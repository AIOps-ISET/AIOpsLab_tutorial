# 知识获取

* 知识获取的目的是从非结构化文本中构造知识图谱，补全已有的知识图谱，发现识别实体和关系。知识获取的主要任务包括**关系抽取**、**知识图谱补全（KGC）和其他的面向实体获取的任务，例如实体识别**和**实体对齐**。

## 知识图谱补全

* 知识图谱补全（KGC）基于知识图谱不完备的问题，为知识图谱增加新的三元组。典型的子任务包括**链路预测**、**实体预测**和**关系预测**。对KGC的初步研究主要集中在学习低维嵌入的方式用于三元组预测，即**基于嵌入的方法(Embedding-based Models)**。然而，基于嵌入的补全方法大多数都没有捕捉到多步的关系。因此，最近的工作转向探索多步的关系路径和整合逻辑规则，分别称为**基于关系路径推理(Relation Path Reasoning)和基于规则的推理(Rule-based Reasoning)**。除此之外，基于强化学习(RL-based Path Finding)和元关系学习(Meta Relational Learning)的方法也有研究。

### 基于嵌入的方法

* 前面提到的KRL方法，例如**TransE**、**TransH**、**TransR**、**HolE**和**R-GCN**，以及联合学习方法，例如**DKRL**，都可以用于KGC中。

### 关系路径推理

* 实体和关系的嵌入学习虽然在一些任务中取得了不错的效果，但是不能对复杂的关系路径建模。关系路径推理利用了图结构中的路径信息。
* 现有研究进展：
  * 随机游走推断已经被广泛研究，例如Path-Ranking算法（PRA）在路径限制额组合下选择了关系路径，并且进行了极大似然分类。
  * 为了改进路径搜索，Gardner等人通过结合文本内容，在随机游走中引入了向量空间相似性的启发式方法，同时缓解了PRA中的特征稀疏性问题。
  * Neelakantan等人提出了RNN模型，通过递归地应用组合性来组合关系路径的内涵。
  * Chain-of-Reasoning是一种支持multiple reasons的神经注意力机制，它表示了跨所有关系、实体和文本的逻辑组合。
  * 最近，DIVA提出了统一的变分推断框架，该框架将多条推理视为两个子过程：1）路径的发现，底层路径推断的先验分布；2）路径推理，用于链接分类的似然。

### 基于规则的推理

* 一个规则由head和body组成，形式化为head ← body 。其中head是一个原子（atom），例如有着可变subject和/或objects的事实；body可以是原子的集合。
* 现有更多的研究关注于将逻辑规则整合到嵌入中，以提高模型的推理能力，例如使用联合学习和迭代训练用于合并一阶逻辑规则。可以使用规则挖掘工具（例如 AMIE）来抽取出逻辑规则。现有研究进展有：
  * KALE提出了统一的联合模型，定义了兼容三元组和逻辑规则嵌入的t-范数（t-norm）模糊逻辑连接词。定义了逻辑合取、析取和否定三种组合，构成complex formula的真值。图 7a展示了简单的一阶Horn clause推断。
  * RUGE提出迭代的模型，使用soft rules从无标注的三元组进行soft的标签预测，从有标签的三元组进行embedding rectification。
  * IterE提出递归的训练策略，有3个部分：嵌入学习；axiom induction；axiom injection。
* 神经模型和符号模型的结合同样吸引了研究者的注意力，其可使用端到端的方式实现基于规则的推理。现有研究进展有：
  * Neural Theorem Provers (NTP) 学习到了用于多条推理的逻辑规则，利用了radial basis function kernel在向量空间上进行可微计算。
  * NeuralLP实现了在归纳式的逻辑编程（inductive logic programming）中使用基于梯度的优化，其中的神经控制系统使用了集成注意力机制和辅助记忆。
  * pLogicNet提出了概率逻辑神经网络，结合Markov逻辑网络和KRL方法的优点，利用了一阶逻辑并学习到了有效的嵌入，同时处理了逻辑规则的不确定性。
  *ExpressGNN对graph networks和嵌入进行微调，对pLogicNet进行了泛化，并实现了更有效的逻辑推理。

## 实体发现

* 实体发现这里主要包含几个任务的细分，即**实体识别**、**实体消歧**、**实体对齐**、**实体类型**。
* **实体识别**是NLP的基础任务，主要的模型是**LSTM**、**CRF**、**MGNER**等。
* **实体类型**包括粗粒度和细粒度类型，而后者使用树形结构类型类别，通常被视为多类别和多标签分类，典型的模型是**PLE**。
* **实体消歧**或**实体链接**是将实体与知识图谱中相应的实体进行链接进而统一的任务，代表模型是**DSRM**、**EDKate**等。上述任务涉及到从文本或单个知识图谱中发现实体，而实体对齐(EA)旨在融合异类知识图谱之间的知识。

下面给出模型论文名称及出处：

* LSTM，IEEE 1997，Long Short-Term Memory

{% embed url="https://pubmed.ncbi.nlm.nih.gov/9377276/" %}

* CRF，ACM, 2001，Conditional R andom Fields: Probabilistic Models for Segmenting and Labeling Sequence Data

{% embed url="https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers" %}

* LSTM-CRF，NAACL 2016,Neural architectures for named entity recognition

{% embed url="https://arxiv.org/abs/1603.01360" %}

* BiLSTM-CRF，Bidirectional LSTM-CRF Models for Sequence Tagging

{% embed url="https://arxiv.org/pdf/1508.01991v1.pdf" %}

* MGNER，ACL 2019，Multi-Grained Named Entity Recognition

{% embed url="https://arxiv.org/abs/1906.08449" %}

* PLE，ACM 2020，Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations

{% embed url="https://dl.acm.org/doi/abs/10.1145/3383313.3412236" %}

## 关系抽取

关系抽取是从纯文本中抽取未知的关系事实并将其加入到知识图谱中，是自动构建大规模知识图谱的关键。目前来说，都在采用神经网络进行关系抽取的研究。

相关研究论文名称：

​* Constructing biological knowledge bases by extracting information from text sources
* Discovering correlations between sparse features in distant supervision for relation extraction
* Relation extraction with multi-instance multi-label convolutional neural networks
* Incorporating relation paths in neural relation extraction
